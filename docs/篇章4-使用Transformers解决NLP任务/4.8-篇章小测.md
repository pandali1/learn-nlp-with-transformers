## 篇章小测
* 问题1: NLP生成任务用什么预训练模型效果更好，BERT、Roberta，GPT，T5？
  * 使用GPT和T5更好，GPT主要以Transformer中的Decoder为基础，T5则以Encoder和Decoder为基础
* 问题2: 多选问答如何构建输入？
  * 将问题和答案进行拼接
* 问题3: 抽取式问答如何解决超长的文本context？
  * 进行切片，并且切片之间存在交叉
  * 将问题分别与切片进行pair，不包含的答案的切片，假设答案为[CLS]对应的token位置
* 问题4: 如何选择tokenizer和pre-trained模型？
  * 根据任务选择pre-trained模型，然后选择与他相对应的tokenizer