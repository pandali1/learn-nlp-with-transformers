## 篇章小测
* 问题1: BERT训练时候的学习率learning rate如何设置?
  * 在 fine-tune阶段使用过大的学习率，会打乱 pretrain 阶段学习到的句子信息，造成“灾难性遗忘”。BERT没有下游微调结构的，是直接用BERT去fine-tune时，BERT模型的训练和微调学习率取2e-5和5e-5效果会好一些。
  * 那如果微调的时候接了更多的结构，比如BERT+TextCNN，BERT+BiLSTM+CRF，此种情况下BERT的fine-tune学习率可以设置为5e-5, 3e-5, 2e-5。
  * 而下游任务结构的学习率可以设置为1e-4，让其比bert的学习更快一些。至于这么做的原因也很简单：BERT本体是已经预训练过的，即本身就带有权重，所以用小的学习率很容易fine-tune到最优点，而下接结构是从零开始训练，用小的学习率训练不仅学习慢，而且也很难与BERT本体训练同步。
  * 为此，我们将下游任务网络结构的学习率调大，争取使两者在训练结束的时候同步：当BERT训练充分时，下游任务结构也能够训练充分。
* 问题2: BERT模型使用哪种分词方式？
  * BasicTokenizer+WordPieceTokenizeer
* 问题3: 如何理解BERT模型输入的type ids？
  * 区别句子与 padding、句子对间的差异
* 问题4: Hugginface代码中的BasicTokenizer作用是？
  * 按标点、空格等分割句子，并处理是否统一小写，以及清理非法字符。
* 问题5: WordPiece分词的好处是什么？
  * 区分英文中的时态、词根词缀、单复数等
  * 减小embedding词典的大小，避免词典爆炸
* 问题6: BERT中的warmup作用是什么？
  * 在训练初期使用较小的学习率（从 0 开始），在一定步数（比如 1000 步）内逐渐提高到正常大小（比如上面的 2e-5），避免模型过早进入局部最优而过拟合；